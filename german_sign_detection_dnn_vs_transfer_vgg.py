# -*- coding: utf-8 -*-
"""German sign detection DNN vs Transfer VGG

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LYlmLjBA4lIvvOAbU5SQf_yrFnE6iM66
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2
from PIL import Image
import os

#load kaggle.json (previously downloaded from my kaggle account)
 !pip install -q kaggle
 from google.colab import files
 files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
!ls -l ~/.kaggle
!cat ~/.kaggle/kaggle.json

#Install kaggle packages
!pip install -q kaggle
!pip install -q kaggle-cli

pwd

import os
#download GTSR dataset
!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign

#check that it is there
ls

#Unzip
import zipfile

zipref = zipfile.ZipFile('gtsrb-german-traffic-sign.zip', 'r')
zipref.extractall('dataset')
zipref.close()

os.chdir('dataset')
!ls -l

print(os.listdir('../dataset'))

# Reading the input images and putting them into a numpy array
data=[]
labels=[]

height = 40
width = 40
channels = 3
classes = 43
n_inputs = height * width*channels

for i in range(classes) :
    path = "../dataset/train/{0}/".format(i)
    print(path)
    Class=os.listdir(path)
    for a in Class:
        try:
            image=cv2.imread(path+a)
            image_from_array = Image.fromarray(image, 'RGB')
            size_image = image_from_array.resize((height, width))
            data.append(np.array(size_image))
            labels.append(i)
        except AttributeError:
            print(" ")
            
Cells=np.array(data)
labels=np.array(labels)

#Randomize the order of the input images
s=np.arange(Cells.shape[0])
np.random.seed(43)
np.random.shuffle(s)
Cells=Cells[s]
labels=labels[s]

#Spliting the images into train and validation sets
PP_Cells = tf.keras.applications.vgg16.preprocess_input(Cells) #preprocess for vgg (for comparison with vgg transfer learning later)
(X_train,X_val)=PP_Cells[(int)(0.2*len(labels)):],PP_Cells[:(int)(0.2*len(labels))]
#(X_train,X_val)=Cells[(int)(0.2*len(labels)):],Cells[:(int)(0.2*len(labels))]
#X_train = X_train.astype('float32')/255 
#X_val = X_val.astype('float32')/255
(y_train,y_val)=labels[(int)(0.2*len(labels)):],labels[:(int)(0.2*len(labels))]

#Using one hot encoding for the train and validation labels
from keras.utils import to_categorical
y_train = to_categorical(y_train, 43)
y_val = to_categorical(y_val, 43)

#Definition of the DNN model

from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout

model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))
model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(43, activation='softmax'))

#Compilation of the model
model.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

model.summary()

from tensorflow.keras.preprocessing.image import ImageDataGenerator
BATCH_SIZE=32

train_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_generator = train_datagen.flow(X_train,
                                    y_train,
                                    batch_size=BATCH_SIZE,
                                    shuffle=True,
                                    seed=17)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)
val_generator = val_datagen.flow(X_val,
                                    y_val,
                                    batch_size=BATCH_SIZE,
                                    shuffle=False,
                                    seed=17)

from timeit import default_timer as timer

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10
#history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_val, y_val))

start_time = timer()
history = model.fit_generator(train_generator,
                    epochs=epochs,
                    verbose=1,
                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                    validation_data=val_generator,
                    steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))
end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

#Predicting with the test data
y_test=pd.read_csv("../dataset/Test.csv")
labels=y_test['Path'].as_matrix()
y_test=y_test['ClassId'].values

data=[]

for f in labels:
    image=cv2.imread('../dataset/test/'+f.replace('Test/', ''))
    image_from_array = Image.fromarray(image, 'RGB')
    size_image = image_from_array.resize((height, width))
    data.append(np.array(size_image))

x_test=np.array(data)
#x_test = x_test.astype('float32')/255 
X_test = tf.keras.applications.vgg16.preprocess_input(x_test)
#X_test = x_test
pred = model.predict_classes(X_test)

#Accuracy with the test data
from sklearn.metrics import accuracy_score
accuracy_score(y_test, pred)

# Confusion matrix 
import itertools
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=75) 
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

class_names = range(43)
cm = confusion_matrix(pred,y_test)

plt.figure(2)
plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')

from sklearn import metrics
print("Classification Report: \n", metrics.classification_report(y_test, pred))
print("Confusion Matrix: \n", metrics.confusion_matrix(y_test, pred))

X_test.shape

pred.shape

pred[2]  #should be 38

#Definition second model - use pretrained VGG
TARGET_SIZE = (height, width) 
IMG_SHAPE = TARGET_SIZE + (3,)
pretrained_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

pretrained_model.summary()

import cv2
from PIL import Image
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose, ZeroPadding2D, Cropping2D, BatchNormalization
from tensorflow.keras.models import Model
from shutil import copyfile, rmtree
from timeit import default_timer as timer
from sklearn.model_selection import train_test_split
from sklearn import metrics

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dense(256, activation='relu')(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_1 = Model(pretrained_model.input, x)

model_tfl_1.summary()

#Compilation of the model
model_tfl_1.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_1.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_1.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Seems to be overfitting to training data. Add dropout layers."""

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.33)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.33)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_2 = Model(pretrained_model.input, x)

model_tfl_2.summary()

#Compilation of the model
model_tfl_2.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_2.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_2.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Still overfitting, increase dropout, add batch normalisation"""

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_3 = Model(pretrained_model.input, x)

model_tfl_3.summary()

#Compilation of the model
model_tfl_3.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_3.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_3.predict(X_test)

#Accuracy with the test data
from sklearn.metrics import accuracy_score
from sklearn import metrics

accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Remove dropout after batchnormalisation layer"""

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_4 = Model(pretrained_model.input, x)

model_tfl_4.summary()

#Compilation of the model
model_tfl_4.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_4.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_4.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))



"""Revert to no BN, but keep both dropouts and increase to 0.5"""

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_5 = Model(pretrained_model.input, x)

model_tfl_5.summary()

#Compilation of the model
model_tfl_5.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_5.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_5.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Two BN's, No Dropout"""

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = BatchNormalization()(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_6 = Model(pretrained_model.input, x)

model_tfl_6.summary()

#Compilation of the model
model_tfl_6.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_6.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
  print('Epochs: ', epochs)
  print('==============================')
  print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
  acc=history.history['acc'][-1]
  test_acc = model.evaluate_generator(val_generator)[1]
    
  print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
          .format(i, acc*100, test_acc*100))

pred = model_tfl_6.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Back to two dropouts, 0.33, and add data augmentation"""

train_aug_datagen = ImageDataGenerator(rescale=1.0/255.0,
                                       rotation_range = 18,
                                       width_shift_range = 0.18,
                                       height_shift_range = 0.18,
                                       shear_range = 0.18,
                                       zoom_range = 0.18,
                                       horizontal_flip = False)

train_aug_generator = train_aug_datagen.flow(X_train,   
                                    y_train,
                                    batch_size=BATCH_SIZE,
                                    shuffle=True,
                                    seed=17)

for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.33)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.33)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_2 = Model(pretrained_model.input, x)

model_tfl_2.summary()

#Compilation of the model
model_tfl_2.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#RETRAIN with DATA AUGMENTATION
#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_2.fit_generator(train_aug_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_2.predict(X_test)

accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Add regularisation"""

from keras import regularizers

last_layer = pretrained_model.get_layer('block4_conv3')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.33)(x)
x = Dense(256, kernel_regularizer=regularizers.l2(0.001), activation='relu')(x)
x = Dropout(0.33)(x)
x = Dense(128, kernel_regularizer=regularizers.l2(0.001), activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_7 = Model(pretrained_model.input, x)

model_tfl_7.summary()

#Compilation of the model
model_tfl_7.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_7.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_7.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Still significant overfitting. Try reducing layers."""

#Modify the model again
for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.33)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.33)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_8 = Model(pretrained_model.input, x)

model_tfl_8.summary()

#Compilation of the model
model_tfl_8.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_8.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=20)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_8.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Increase dropout and add BN"""

#Modify the model again
for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_9 = Model(pretrained_model.input, x)

model_tfl_9.summary()

#Compilation of the model
model_tfl_9.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_9.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_9.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Add another batch norm"""

#Modify the model again
for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_10 = Model(pretrained_model.input, x)

model_tfl_10.summary()

#Compilation of the model
model_tfl_10.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_10.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_10.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Remove both BN's, leave dropouts"""

#Modify the model again
for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_11 = Model(pretrained_model.input, x)

model_tfl_11.summary()

#Compilation of the model
model_tfl_11.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_11.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_11.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Looks like the original BN is needed. Put back in and remove dropouts"""

#Modify the model again
for layer in pretrained_model.layers:
    layer.trainable = False

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_12 = Model(pretrained_model.input, x)

model_tfl_12.summary()

#Compilation of the model
model_tfl_12.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_12.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_12.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""Need BN and Dropout to avoid overfitting
=>best result so far is model_tfl_9. Try with more epochs
"""

#Repeat with more epochs
#using ten epochs for the training and saving the accuracy for each epoch
epochs = 100

start_time = timer()
history = model_tfl_9.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_9.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

print("Classification Report: \n", metrics.classification_report(y_test, np.argmax(pred, axis = 1)))
print("Confusion Matrix: \n", metrics.confusion_matrix(y_test, np.argmax(pred, axis = 1)))

"""What benefit are we getting from the pretraining? Try same network without pretrained weights"""

#Modify the model again (same as model_tfl_9 but making VGG layers trainable)
for layer in pretrained_model.layers:
    layer.trainable = True

last_layer = pretrained_model.get_layer('block2_pool')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output

x = Flatten()(last_output)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dense(43, activation='softmax')(x)
model_tfl_13 = Model(pretrained_model.input, x)

model_tfl_13.summary()

#Compilation of the model
model_tfl_13.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#using ten epochs for the training and saving the accuracy for each epoch
epochs = 10

start_time = timer()
history = model_tfl_13.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_13.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""It seems pretraining the early layers with imagenet improves generalisation. repeat with 100 epochs for comparison"""

epochs = 100

start_time = timer()
history = model_tfl_13.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()



print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_13.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))

"""The last FC-4096 layer of VGG is supposed to generalise well, so lets try one more test using this pretrained layer"""

import cv2
from PIL import Image
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose, ZeroPadding2D, Cropping2D, BatchNormalization
from tensorflow.keras.models import Model
from shutil import copyfile, rmtree
from timeit import default_timer as timer
from sklearn.model_selection import train_test_split
from sklearn import metrics

#Definition second model - use pretrained VGG
#TARGET_SIZE = (height, width) 
#IMG_SHAPE = TARGET_SIZE + (3,)
full_pretrained_model = tf.keras.applications.VGG16(input_shape=(224, 224, 3),
                                               include_top=True,
                                               weights='imagenet')

full_pretrained_model.summary()

#Modify the model again 
for layer in full_pretrained_model.layers:
    layer.trainable = False

last_layer = full_pretrained_model.get_layer('fc2')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output


x = Dense(43, activation='softmax')(last_output)
model_tfl_14 = Model(full_pretrained_model.input, x)

model_tfl_14.summary()

#Compilation of the model
model_tfl_14.compile(
    loss='categorical_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']
)

#Need to re-read dataset to match 224,224,3 target size for fc2 layer
# Reading the input images and putting them into a numpy array
data=[]
labels=[]

height = 224
width = 224
channels = 3
classes = 43
n_inputs = height * width*channels

for i in range(classes) :
    path = "../dataset/train/{0}/".format(i)
    print(path)
    Class=os.listdir(path)
    for a in Class:
        try:
            image=cv2.imread(path+a)
            image_from_array = Image.fromarray(image, 'RGB')
            size_image = image_from_array.resize((height, width))
            data.append(np.array(size_image))
            labels.append(i)
        except AttributeError:
            print(" ")
            
Cells=np.array(data)
labels=np.array(labels)

#Randomize the order of the input images
s=np.arange(Cells.shape[0])
np.random.seed(43)
np.random.shuffle(s)
Cells=Cells[s]
labels=labels[s]

#Spliting the images into train and validation sets
PP_Cells = tf.keras.applications.vgg16.preprocess_input(Cells) #preprocess for vgg (for comparison with vgg transfer learning later)
(X_train,X_val)=PP_Cells[(int)(0.2*len(labels)):],PP_Cells[:(int)(0.2*len(labels))]
#(X_train,X_val)=Cells[(int)(0.2*len(labels)):],Cells[:(int)(0.2*len(labels))]
#X_train = X_train.astype('float32')/255 
#X_val = X_val.astype('float32')/255
(y_train,y_val)=labels[(int)(0.2*len(labels)):],labels[:(int)(0.2*len(labels))]

#Using one hot encoding for the train and validation labels
from keras.utils import to_categorical
y_train = to_categorical(y_train, 43)
y_val = to_categorical(y_val, 43)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
BATCH_SIZE=32

train_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_generator = train_datagen.flow(X_train,
                                    y_train,
                                    batch_size=BATCH_SIZE,
                                    shuffle=True,
                                    seed=17)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)
val_generator = val_datagen.flow(X_val,
                                    y_val,
                                    batch_size=BATCH_SIZE,
                                    shuffle=False,
                                    seed=17)

from timeit import default_timer as timer
epochs = 10

start_time = timer()
history = model_tfl_14.fit_generator(train_generator,
                        epochs=epochs,
                        verbose=1,
                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='acc', min_delta=0.0001, patience=2)],
                        validation_data=val_generator,
                        steps_per_epoch= round(X_train.shape[0] / BATCH_SIZE))

end_time = timer()

#Display of the accuracy and the loss values
import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(history.history['acc'], label='training accuracy')
plt.plot(history.history['val_acc'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()

plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()

print('==============================')
print('Epochs: ', epochs)
print('==============================')
print('Trained in {0:.2f} minutes'.format((end_time - start_time) / 60))
    
acc=history.history['acc'][-1]
test_acc = model.evaluate_generator(val_generator)[1]
    
print('Results at the end of training: acc={1:.02f}%, test_acc={2:.02f}%'
        .format(i, acc*100, test_acc*100))

pred = model_tfl_14.predict(X_test)
accuracy_score(y_test, np.argmax(pred, axis = 1))